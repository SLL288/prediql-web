def ensure_ollama_running(model="llama3"):
    # Stubbed for backend: using API-based LLMs instead of local Ollama
    print("Ollama stub: backend uses API-based LLM (OpenAI/Gemini)")
